import os
import cv2
import imgaug.augmenters as iaa

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from pathlib import Path

from keras.models import Sequential, Model
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.layers import Dense, Dropout
from keras.optimizers import Adam, SGD, RMSprop
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from keras.utils import to_categorical



training_data = Path('training/')
validation_data = Path('validation/')
labels_path = Path('monkey_labels.txt')

labels_info = []

# Read the file
lines = labels_path.read_text().strip().splitlines()[1:]
for line in lines:
    line = line.split(',')
    line = [x.strip(' \n\t\r') for x in line]
    line[3], line[4] = int(line[3]), int(line[4])
    line = tuple(line)
    labels_info.append(line)

# Convert the data into a pandas dataframe
labels_info = pd.DataFrame(labels_info, columns=['Label', 'Latin Name', 'Common Name',
                                                 'Train Images', 'Validation Images'], index=None)
# Sneak peek
labels_info.head(10)


# Create a dictionary to map the labels to integers
labels_dict= {'n0':0, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'n5':5, 'n6':6, 'n7':7, 'n8':8, 'n9':9}

# map labels to common names
names_dict = dict(zip(labels_dict.values(), labels_info["Common Name"]))
print(names_dict)

# Creating a dataframe for the training dataset
train_df = []
for folder in os.listdir(training_data):
    # Define the path to the images
    imgs_path = training_data / folder

    # Get the list of all the images stored in that directory
    imgs = sorted(imgs_path.glob('*.jpg'))

    # Store each image path and corresponding label
    for img_name in imgs:
        train_df.append((str(img_name), labels_dict[folder]))

train_df = pd.DataFrame(train_df, columns=['image', 'label'], index=None)
# shuffle the dataset
train_df = train_df.sample(frac=1.).reset_index(drop=True)

####################################################################################################

# Creating dataframe for validation data in a similar fashion
valid_df = []
for folder in os.listdir(validation_data):
    imgs_path = validation_data / folder
    imgs = sorted(imgs_path.glob('*.jpg'))
    for img_name in imgs:
        valid_df.append((str(img_name), labels_dict[folder]))

valid_df = pd.DataFrame(valid_df, columns=['image', 'label'], index=None)
# shuffle the dataset
valid_df = valid_df.sample(frac=1.).reset_index(drop=True)

####################################################################################################

# How many samples do we have in our training and validation data?
print("Number of traininng samples: ", len(train_df))
print("Number of validation samples: ", len(valid_df))

# sneak peek of the training and validation dataframes
print("\n", train_df.head(), "\n")
print("=================================================================\n")
print("\n", valid_df.head())

# Creating a dataframe for the training dataset
train_df = []
for folder in os.listdir(training_data):
    # Define the path to the images
    imgs_path = training_data / folder

    # Get the list of all the images stored in that directory
    imgs = sorted(imgs_path.glob('*.jpg'))

    # Store each image path and corresponding label
    for img_name in imgs:
        train_df.append((str(img_name), labels_dict[folder]))

train_df = pd.DataFrame(train_df, columns=['image', 'label'], index=None)
# shuffle the dataset
train_df = train_df.sample(frac=1.).reset_index(drop=True)

####################################################################################################

# Creating dataframe for validation data in a similar fashion
valid_df = []
for folder in os.listdir(validation_data):
    imgs_path = validation_data / folder
    imgs = sorted(imgs_path.glob('*.jpg'))
    for img_name in imgs:
        valid_df.append((str(img_name), labels_dict[folder]))

valid_df = pd.DataFrame(valid_df, columns=['image', 'label'], index=None)
# shuffle the dataset
valid_df = valid_df.sample(frac=1.).reset_index(drop=True)

####################################################################################################

# How many samples do we have in our training and validation data?
print("Number of traininng samples: ", len(train_df))
print("Number of validation samples: ", len(valid_df))

# sneak peek of the training and validation dataframes
print("\n", train_df.head(), "\n")
print("=================================================================\n")
print("\n", valid_df.head())




# some constants(not truly though!)

# dimensions to consider for the images
img_rows, img_cols, img_channels = 224,224,3

# batch size for training
batch_size=8

# total number of classes in the dataset
nb_classes=10



# Augmentation sequence
seq = iaa.OneOf([
    iaa.Fliplr(), # horizontal flips
    iaa.Affine(rotate=20), # roatation
    iaa.Multiply((1.2, 1.5))]) #random brightness


def data_generator(data, batch_size, is_validation_data=False):
    # Get total number of samples in the data
    n = len(data)
    nb_batches = int(np.ceil(n / batch_size))

    # Get a numpy array of all the indices of the input data
    indices = np.arange(n)

    # Define two numpy arrays for containing batch data and labels
    batch_data = np.zeros((batch_size, img_rows, img_cols, img_channels), dtype=np.float32)
    batch_labels = np.zeros((batch_size, nb_classes), dtype=np.float32)

    while True:
        if not is_validation_data:
            # shuffle indices for the training data
            np.random.shuffle(indices)

        for i in range(nb_batches):
            # get the next batch
            next_batch_indices = indices[i * batch_size:(i + 1) * batch_size]

            # process the next batch
            for j, idx in enumerate(next_batch_indices):
                img = cv2.imread(data.iloc[idx]["image"])
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                label = data.iloc[idx]["label"]

                if not is_validation_data:
                    img = seq.augment_image(img)

                img = cv2.resize(img, (img_rows, img_cols)).astype(np.float32)
                batch_data[j] = img
                batch_labels[j] = to_categorical(label, num_classes=nb_classes)

            batch_data = preprocess_input(batch_data)
            yield batch_data, batch_labels





#training data generator
train_data_gen = data_generator(train_df, batch_size)

# validation data generator
valid_data_gen = data_generator(valid_df, batch_size, is_validation_data=True)




# simple function that returns the base model
def get_base_model():
    base_model = VGG16(input_shape=(img_rows, img_cols, img_channels), weights='imagenet', include_top=True)
    return base_model




# get the base model
base_model = get_base_model()

#  get the output of the second last dense layer
base_model_output = base_model.layers[-2].output

# add new layers
x = Dropout(0.7,name='drop2')(base_model_output)
output = Dense(10, activation='softmax', name='fc3')(x)

# define a new model
model = Model(base_model.input, output)

# Freeze all the base model layers
for layer in base_model.layers[:-1]:
    layer.trainable=False

# compile the model and check it
optimizer = RMSprop(0.001)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
model.summary()





# always user earlystopping
# the restore_best_weights parameter load the weights of the best iteration once the training finishes
es = EarlyStopping(patience=10, restore_best_weights=True)

# checkpoint to save model
chkpt = ModelCheckpoint(filepath="model1", save_best_only=True)

# number of training and validation steps for training and validation
nb_train_steps = int(np.ceil(len(train_df)/batch_size))
nb_valid_steps = int(np.ceil(len(valid_df)/batch_size))

# number of epochs
nb_epochs=100





# train the model
history1 = model.fit_generator(train_data_gen,
                              epochs=nb_epochs,
                              steps_per_epoch=nb_train_steps,
                              validation_data=valid_data_gen,
                              validation_steps=nb_valid_steps,
                              callbacks=[es,chkpt])





# let's plot the loss and accuracy

# get the training and validation accuracy from the history object
train_acc = history1.history['acc']
valid_acc = history1.history['val_acc']

# get the loss
train_loss = history1.history['loss']
valid_loss = history1.history['val_loss']

# get the number of entries
xvalues = np.arange(len(train_acc))

# visualize
f,ax = plt.subplots(1,2, figsize=(10,5))
ax[0].plot(xvalues, train_loss)
ax[0].plot(xvalues, valid_loss)
ax[0].set_title("Loss curve")
ax[0].set_xlabel("Epoch")
ax[0].set_ylabel("loss")
ax[0].legend(['train', 'validation'])

ax[1].plot(xvalues, train_acc)
ax[1].plot(xvalues, valid_acc)
ax[1].set_title("Accuracy")
ax[1].set_xlabel("Epoch")
ax[1].set_ylabel("accuracy")
ax[1].legend(['train', 'validation'])

plt.show()




# What is the final loss and accuracy on our validation data?
valid_loss, valid_acc = model.evaluate_generator(valid_data_gen, steps=nb_valid_steps)
print("accuracy",valid_acc)
print("loss",valid_loss)